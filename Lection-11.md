# Лекция 11

## Содержание

* TCP
  * Алгоритм Нейгла
  * TCP Timestamps
  * Перегрузка сети
  * TCP Tahoe
    * Управление перегрузками
    * Динамическое определение таймаутов
    * Самосинхронизация
  * Keep-alive
  * Zero-window-probe
* [PGM](https://github.com/AliakseiSuvorau/networks/blob/master/Lection-11.md#pgm-pragmatic-general-multicast)
  * Задачи
  * Борьба с потерей пакетов

### Алгоритм Нейгла

* Пусть будем посылать tiny-граммы. Их много, поэтому получим перегрузку сети
* Алгоритм Нейгла позволяет бороться с перегрузкой сети небольшими пакетами
* Алгоритм может вызвать проблемы с производительностью у некоторых приложений, которым важно передавать все без задержек
* Алгоритм можно отключить (TCP_NODELAY в опциях)
* Алгоритм задерживает маленькие пакеты в буфере и объединяет их
* **Алгоритм**:
  1) Если окно позволяет и данных не меньше, чем MSS, то можем посылать
  2) Иначе, если в "потоке" есть неподтвержденные данные, то буферизуем их, пока не получим подтверждение
  3) Иначе, отправляем немедленно

> [!NOTE]
> Из-за пункта 2 алгоритма может быть delay.

> [!NOTE]
> Если сообщение длинное, то режем его по максимальному размеру фрагмента.

### TCP Timestamps

* Указываем "время" посылки, оно инкрементируется
* Дублируем эту метку в ответе
* Помогает сопоставить вопрос и ответ при переполнении SEQ-счетчика

### Перегрузка сети

* *Ситуация*: имеем жирное соединения, но есть статистическое мультиплексирование (т.е. по этому пути еще кто-то может передавать данные), поэтому не имеем указанного
  объема пропускания
  * Буфер переполняется
  * При перегрузке теряем пакеты => теряем ресурсы сети => нужно избегать

* Борьба с перегрузками сети: RFC 5681 (2009 г.)
  * Медленный старт и избегание перегрузки
  * Быстрая перепосылка и быстрое восстановление
  * Таймауты
    * Ждем, если получили не полный MSS, прежде чем посылаем подтверждение (для уменьшения количества таких сообщений) - до 500 миллисекунд
    * Restarting Idle Connections - сброс накопленного размера окна, если долго не общались с товарищем (т.к. за это время сетка могла поменяться)
  * При перегрузке (потеряли пакет) уменьшаем размер окна

* В итоге:
  * Реализация на уровне хоста
  * Причина: потеря пакета
  * Можно использовать механизмы скользящего окна

* Подход AIMD (Additive Increase, Multiplicative Decrease)
  * В TCP **не** используется, но используется идея:
    * Пакет доставили => $w = w + \frac{1}{w}$
    * Пакет потерян => $w = \frac{w}{2}$

### TCP Tahoe (1987)

* Управление перегрузками
* Динамическое определение таймаутов
* Самосинхронизация

#### Управление перегрузками

* Медленный старт
  * Каждый подтвержденный сегмент увеличивает размер окна перегрузок на MSS. При превышении порога sstreshold переходит в режим избегания перегрузок
* Избегание перегрузок
  * Каждый подтвержденный сегмент увеличивает окно перегрузок (cwnd) на $\frac{\text{MSS}^2}{\text{cwnd}}$
  * при достижении таймаута или тройного подтверждения одного и того же сегмента меняет $\text{sstresh}=\frac{\text{cwnd}}{2}$ и $\text{cwnd}=\text{MSS}$ и переходит в
    режим медленного старта

#### Динамическое определение таймаутов

* Нужны для определения времени, по истечении которого считаем, что сегмент потерян. Тогда должны уменьшить размер окна
* Основной алгоритм:
  * Измеряем RTT
  * $\text{EstRTT}=\text{a}\cdot\text{EstRTT}+\text{b}\cdot\text{SampleRTT}$
    * $\text{a}+\text{b}=1$
    * $\text{a}\in\[0.8,0.9\]$
    * $\text{b}\in\[0.1,0.2\]$
  * $\text{timeout}=2\cdot\text{EstRTT}$
* Алгоритма Карна-Патриджа
  * Не считать RTT во время перепосылки
  * После каждой перепосылки удваиваем таймаут
* Алгоритм Якобсона-Карела
  * $\text{difference}=\text{SampleRTT}-\text{EstRTT}$
  * $\text{EstRTT}=\text{EstRTT}+(\text{d}\cdot\text{difference})$
  * $\text{deviation}=\text{deviation}+\text{d}(|\text{difference}|-\text{deviation}))$, где $\text{d}\in\(0,1)$
  * Учитываем дисперсию, когда считаем таймаут:
    * $\text{timeout}=\text{u}\cdot\text{EstRTT}+\text{q}\cdot\text{deviation}$, где $\text{u}=1$ и $\text{q}=4$
  
#### Самосинхронизация

* Посылка данных при получении подтверждения
* Агрессивная политика

### Keep-alive

* *Ситуация*:
  * Установили соединение
  * Обмениваемся данными
  * Встаем на паузу (никто ничего не посылает)
  * Как понять, что клиент не умер? Никак!

* *Решение*:
  * Посылаем специальный пакет с SEQ меньшим на единицу
  * Сервер в ответ тоже посылает подтверждение

> [!NOTE]
> По умолчанию алгоритм начнет работать (т.е. пошлет первый такой пакет) через 2 часа. Этот параметр можно менять при настройке сокета.

### Zero-window-probe

* *Ситуация*:
  * Шлем много данных
  * Партнер перенасытился, данные обработать не может, складывает их в буфер => окно стало равным нулю
  * Когда партнер обработает пакет, то пошле подтверждение с увеличением окна
  * Но что если потеряем подтверждение?

* *Решение*: "потыкать палочкой"
  * Посылаем пакет с вопросом "увеличилось ли окно?"

* RFC 7323, RFC 5981

> [!NOTE]
> Explicit conjection Notification - можно настроить через флаги в заголовке ip или tcp. В tcp - ECE перед URG и CWR (т.е. Recieved) перед ECE.

## PGM (Pragmatic General Multicast)

* RFC 3208

### Задачи

* Доставка от множества источников множеству получателей
* Надежность (можем понять, что что-то пошло не так)
  * Можем посылать неупорядоченные данные

> [!NOTE]
> Нужно все, что и в IGMP, но получаем надежность.

### Борьба с потерей пакетов

* Подход TCP с подтверждением плох, т.к. получателей может быть много
* *Вывод*: используем негативные подтверждения, т.е. посылаем подтверждение о том, что мы **не** получили (NAck)
* Как узнаем, что пропустили?
  * Получаем следующую через одну порцию данных
  * Получаем сообщение о том, что уже было послано и у себя не все находим
* источник держит данные в буфере некоторое время

* *Проблема*: все равно может быть ситуация, когда пакет отвалился в самом начале пути => получим NAck от всех (такое называется NAck Implosion)
* *Решение*: добавим маршрутизатору понимание PGM
  * Используем SPMs (Source Path Messages) - запоминаем, на какой узел будем посылать жалобу, если вдруг что-то не дошло
  * На каждую жалобу получаем подтверждение
  * Источник получает жалобу и перепосылает данные
  * Если устройство (маршрутизатор) слишком глупое, чтобы подтвердить SPM, то этот SPM проскочит дальше и мы получим подтверждение от следующего за ним более
    умного устройства. Те хосты, которые не посылали жалобу, но получили подтверждение на какую-то жалобу, просто игнорируют его
* Имеем:
  * От источника до получателя:
    * SPM
    * DATA
    * NCF
  * От получателя до источника:
    * NAck
